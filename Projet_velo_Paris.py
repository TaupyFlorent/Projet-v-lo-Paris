import sys
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import io


@st.cache_data
def lire_csv(fichier_csv, sep):
    df = pd.read_csv(fichier_csv, sep=sep)
    return df

df = lire_csv('comptage-velo-donnees-compteurs.csv', sep=';')


st.title("Comptage cycliste Paris")
st.sidebar.title("Sommaire")
pages=["Le Projet", "Jeux de donn√©es", "DataVizualisation", 'Machine Learning', 'Conclusion']
page=st.sidebar.radio("Aller vers", pages)

if page == pages[0]:
    st.write("## üö¥‚Äç‚ôÇÔ∏è Le Projet üö¥‚Äç‚ôÄÔ∏è")
    
    st.write("""
    Depuis plusieurs ann√©es, la **Ville de Paris** a entrepris le d√©ploiement de **compteurs √† v√©lo permanents** sur ses pistes cyclables. Ces dispositifs enregistrent le nombre de cyclistes passant par heure et par piste, fournissant des **donn√©es essentielles** pour √©valuer l'utilisation du r√©seau cyclable parisien.
    
    ### Objectif Principal
    L'objectif principal de ce projet est d'obtenir une vision pr√©cise de la **pratique du cyclisme √† Paris**, permettant ainsi de guider les investissements financiers et d'am√©nager efficacement l'infrastructure urbaine.
    
    ### Importance du Projet
    Promouvoir des **modes de d√©placement √©cologiques** est une priorit√© pour la mairie de Paris. En cons√©quence, ce projet, en cours depuis plusieurs ann√©es, rev√™t une importance capitale pour le **d√©veloppement durable** de la ville.
    
    ### Utilisation des Donn√©es
    Les donn√©es recueillies sont cruciales pour orienter les d√©cisions **politiques et financi√®res**, visant √† am√©liorer la **qualit√© de vie des Parisiens** tout en soutenant la **transition vers des modes de transport plus respectueux de l'environnement**.
    """)
    
    st.markdown("""
    **Points Cl√©s du Projet :**
    - üåø **D√©veloppement Durable** : Soutenir la transition √©cologique.
    - üö≤ **Infrastructure Cyclable** : Am√©liorer et √©tendre les pistes cyclables.
    - üìä **Donn√©es Pr√©cieuses** : Informer les d√©cisions politiques et financi√®res.
    - üèôÔ∏è **Qualit√© de Vie** : Am√©liorer la vie quotidienne des Parisiens.
    """)

    st.write("### üó∫Ô∏è Impact sur la Ville")
    st.write("Ce projet permet de mieux comprendre et d'analyser les flux de cyclistes, facilitant ainsi la planification urbaine et les investissements futurs dans les infrastructures de transport.")


if page == pages[1]:
    st.write("## üìä Jeux de Donn√©es")

    st.write("""
    L'objectif de cette section est de vous pr√©senter le **jeu de donn√©es de base** sur lequel nous avons travaill√©. Nous allons :
    - **Corriger** et **optimiser** ce dataset.
    - Introduire les **donn√©es m√©t√©orologiques** que nous avons ajout√©es pour approfondir l'analyse.

    Ces √©tapes sont cruciales pour garantir la pr√©cision et la pertinence de nos conclusions.
    """)

    st.write("""
    La **Ville de Paris** utilise des **compteurs v√©lo permanents** pour √©valuer l'usage des pistes cyclables. Selon l'am√©nagement, un site de comptage peut √™tre √©quip√© de :
    - **Un compteur** pour les pistes cyclables unidirectionnelles.
    - **Deux compteurs** pour les pistes cyclables bidirectionnelles.

    ### Description du Jeu de Donn√©es
    Ce jeu de donn√©es contient les comptages v√©lo horaires sur une p√©riode glissante de **13 mois** (J-13 mois), mis √† jour quotidiennement (J-1). Les compteurs sont install√©s :
    - Sur des **pistes cyclables**.
    - Dans certains **couloirs bus ouverts aux v√©los**.

    **Remarque** : Les autres v√©hicules (ex. : trottinettes) ne sont pas compt√©s.

    ### Source des Donn√©es
    Les donn√©es sont fournies quotidiennement via l'API de notre partenaire **Eco Compteur**. Comme l'API ne fournit pas nativement le comptage par sens, un traitement par agr√©gation a √©t√© effectu√© pour le reconstituer.

    ### D√©tails Inclus
    - **Identifiant du compteur**
    - **Nom du compteur**
    - **Identifiant du site de comptage**
    - **Nom du site de comptage**
    - **Comptage horaire**
    - **Date et heure de comptage**
    - **Date d'installation du site de comptage**
    - **Lien vers photo du site de comptage**
    - **Coordonn√©es g√©ographiques**
    """)

    afficher_dataset = st.checkbox('Afficher le dataset')

    # Si la case est coch√©e, afficher le dataset
    if afficher_dataset:
        st.write("### Dataset")
        st.dataframe(df.head())

    st.write("### Info Dataset")
    st.write("Voici les infos sur le dataset de base :")
    buffer = io.StringIO()
    df.info(buf=buffer)
    info_str = buffer.getvalue()
    # Display the captured info
    st.text(info_str)

    st.write("### Correction du Dataset")
    st.write("Apr√®s analyse du Dataset, je d√©cide de supprimer les colonnes suivantes qui me paraissent inutiles :")
    st.code("""
    df = df.drop(columns=[
    'Identifiant du site de comptage', 
    'Nom du site de comptage', 
    'Date d\'installation du site de comptage',
    'Lien vers photo du site de comptage', 
    'Identifiant technique compteur',
    'ID Photos', 
    'test_lien_vers_photos_du_site_de_comptage_',
    'id_photo_1', 
    'url_sites', 
    'type_dimage', 
    'mois_annee_comptage'
    ], axis=1)
    """)

    df2 = lire_csv('df3_paris_velo.csv', sep=',')

    st.write("Je d√©cide ensuite de renommer certaines colonnes, de s√©parer certaines colonnes en deux pour faciliter ensuite mon analyse et pouvoir gagner en pr√©cision")
    st.write("Je rajoute ensuite des donn√©es m√©t√©o pour pouvoir affiner mon analyse. Voici le DataSet df_meteo :")

    df_meteo = lire_csv('df_meteo.csv', sep=',')
    df_merged_meteo = lire_csv('df_merged_meteo.csv', sep=',')

    st.write("""
    #### Renommage et Transformation des Colonnes

    Pour faciliter notre analyse et gagner en pr√©cision, nous avons renomm√© certaines colonnes et s√©par√© certaines d'entre elles en deux. Cela nous permettra d'examiner plus en d√©tail les diff√©rentes facettes des donn√©es.

    #### Ajout des Donn√©es M√©t√©o

    Afin d'affiner notre analyse, nous avons int√©gr√© des donn√©es m√©t√©orologiques. Voici un aper√ßu du DataFrame contenant ces informations :
    """)

    st.write(df_meteo.head())

    st.write("""
    **Description des colonnes :**
    - **RR1** : Quantit√© de pluie tomb√©e (en mm).
    - **T** : Temp√©rature relev√©e heure par heure (en ¬∞C).

    Ces donn√©es m√©t√©orologiques nous permettront de mieux comprendre l'impact des conditions climatiques sur la pratique du cyclisme √† Paris.
    """)

    st.write("""
    ### DataSet Final

    Apr√®s avoir fusionn√© et modifi√© les donn√©es, nous obtenons le dataset final qui sera utilis√© pour le projet de machine learning. Voici un aper√ßu de ce dataset :
    """)

    st.write(df_merged_meteo.head())

    st.write("""
    **Remarque** : Les colonnes relatives au mois, au jour et √† l'heure ont √©t√© transform√©es √† l'aide de fonctions sinus/cosinus pour capturer leur caract√®re cyclique, am√©liorant ainsi l'efficacit√© de nos mod√®les de machine learning.
    """)

if page == pages[2]:
    st.write("## üö¥‚Äç‚ôÇÔ∏è Data Visualization üö¥‚Äç‚ôÄÔ∏è")

    st.write("#### Analyse g√©ographique")

    st.write("""
    L'objectif de cette section est de pr√©senter divers graphiques illustrant l'impact de diff√©rentes donn√©es sur le comptage des cyclistes √† Paris.
    Nous allons commencer par analyser la r√©partition du traffic d'un point de vue g√©ographique.
    """)

    df3 = lire_csv('df3.csv', sep=',')

    import pydeck as pdk

    # Fusionner les donn√©es en sommant les valeurs de comptage pour chaque latitude et longitude
    df_aggregated = df3.groupby(['Latitude', 'Longitude'], as_index=False).agg({'Comptage': 'sum'})

    # Normaliser les tailles de cercles
    max_comptage = df_aggregated['Comptage'].max()
    min_comptage = df_aggregated['Comptage'].min()

    # Pour √©viter des cercles de taille nulle, ajouter un petit minimum
    df_aggregated['Normalized_Comptage'] = df_aggregated['Comptage'].apply(lambda x: (x - min_comptage) / (max_comptage - min_comptage) * 1000 + 10)

    # D√©finir le centre de la carte
    map_center = [48.8566, 2.3522]  # Centre de Paris

    # Cr√©er la couche de cercles
    layer = pdk.Layer(
        "ScatterplotLayer",
        df_aggregated,
        pickable=True,
        opacity=0.8,
        stroked=True,
        filled=True,
        radius_scale=1,  # Ajustez ce facteur pour changer l'√©chelle des cercles
        radius_min_pixels=5,  # Taille minimale des cercles en pixels
        radius_max_pixels=100,  # Taille maximale des cercles en pixels
        line_width_min_pixels=1,
        get_position='[Longitude, Latitude]',
        get_radius='Normalized_Comptage',  # Utilisez les valeurs normalis√©es pour d√©finir le rayon
        get_fill_color='[255, 0, 0, 140]',  # Couleur des cercles
        get_line_color=[0, 0, 0],  # Couleur des bords des cercles
    )

    # Cr√©er la vue de la carte
    view_state = pdk.ViewState(
        longitude=map_center[1],
        latitude=map_center[0],
        zoom=12,  # Ajustez le niveau de zoom pour Paris
        pitch=0,
    )

    # Rendre la carte
    r = pdk.Deck(layers=[layer], initial_view_state=view_state, tooltip={"text": "{Comptage} cyclistes"})

    # Afficher la carte dans Streamlit
    st.pydeck_chart(r)

    st.write("""
    On observe une concentration plus √©lev√©e de cyclistes le long des boulevards p√©riph√©riques dans le sud de Paris ainsi que dans le centre-ville, particuli√®rement pr√®s des quais de la Seine.""")

    st.write("#### Analyse par mois")

    st.write("""
    Nous allons maintenant analyser le trafic v√©lo en fonction des mois.
    """)

    import matplotlib.pyplot as plt
    import calendar

    # Lecture des fichiers CSV
    df_meteo = lire_csv('df_meteo.csv', sep=',')
    df_merged_meteo = lire_csv('df_merged_meteo.csv', sep=',')

    # S'assurer que la colonne datetime est au format datetime
    df_merged_meteo['datetime'] = pd.to_datetime(df_merged_meteo['datetime'])

    # Grouper les donn√©es par mois et calculer le total de cyclistes pour chaque mois
    df_merged_meteo['Mois'] = df_merged_meteo['datetime'].dt.to_period('M')
    monthly_counts = df_merged_meteo.groupby('Mois')['Comptage'].sum().reset_index()

    # Obtenir les noms des mois
    monthly_counts['Month_Name'] = monthly_counts['Mois'].dt.month.apply(lambda x: calendar.month_name[x])

    # Cr√©er le graphique en barres
    fig, ax = plt.subplots(figsize=(24, 12))  # Agrandir encore plus la taille de la figure
    ax.bar(monthly_counts['Month_Name'], monthly_counts['Comptage'], color='skyblue')
    ax.set_xlabel('Mois', fontsize=20)  # Augmenter la taille de la police du label x
    ax.set_ylabel('Nombre', fontsize=20)  # Augmenter la taille de la police du label y
    ax.set_title('Nombre Total de Cyclistes par Mois', fontsize=24)  # Augmenter la taille de la police du titre

    # Ajuster la taille des √©tiquettes des axes x et y
    ax.tick_params(axis='x', labelsize=16)
    ax.tick_params(axis='y', labelsize=16)

    plt.xticks(rotation=45)
    plt.tight_layout()
    
    # Afficher le graphique dans Streamlit
    st.pyplot(fig)

    st.write("""
    Nous nous apercevons que les mois ont un impact significatif sur le trafic cyclable √† Paris. 
    Il appara√Æt que les mois ensoleill√©s (hors grandes vacances scolaires) sont plus propices √† l'utilisation du v√©lo.
    """)

    st.write("#### Analyse par jour")

    st.write("""
    Nous allons maintenant analyser le trafic v√©lo en fonction du jour de la semaine.
    """)

    # Assurez-vous que la colonne de date/heure est au format datetime
    df_merged_meteo['datetime'] = pd.to_datetime(df_merged_meteo['datetime'])

    # Extraire le jour de la semaine de la colonne datetime (0 = lundi, 6 = dimanche)
    df_merged_meteo['Jour'] = df_merged_meteo['datetime'].dt.dayofweek

    # Grouper les donn√©es par jour et calculer le total de cyclistes pour chaque jour
    daily_counts = df_merged_meteo.groupby('Jour')['Comptage'].sum().reset_index()

    # Obtenez les noms des jours de la semaine
    daily_counts['Day_Name'] = daily_counts['Jour'].apply(lambda x: calendar.day_name[x])

    # Cr√©ez le graphique en barres avec des ajustements pour les titres et labels
    fig, ax = plt.subplots(figsize=(24, 12))  # Agrandir encore plus la taille de la figure
    ax.bar(daily_counts['Day_Name'], daily_counts['Comptage'], color='skyblue')
    ax.set_xlabel('Jour de la semaine', fontsize=20)  # Augmenter la taille de la police du label x
    ax.set_ylabel('Nombre', fontsize=20)  # Augmenter la taille de la police du label y
    ax.set_title('Nombre Total de Cyclistes par Jour de la Semaine', fontsize=24)  # Augmenter la taille de la police du titre

    # Ajuster la taille des √©tiquettes des axes x et y
    ax.tick_params(axis='x', labelsize=16)
    ax.tick_params(axis='y', labelsize=16)

    plt.xticks(rotation=45)
    plt.tight_layout()

    # Afficher le graphique dans Streamlit
    st.pyplot(fig)

    st.write("")
    st.write("")

    st.write("Nous observons que le nombre de cyclistes varie en fonction du jour de la semaine. Les jours de semaine montrent g√©n√©ralement une augmentation du nombre de cyclistes par rapport aux week-ends.")

    st.write("#### Analyse par heure")

    st.write("""
    Nous allons maintenant analyser le trafic v√©lo en fonction de l'heure.
    """)

    # Extraire l'heure de la colonne datetime
    df_merged_meteo['Heure'] = df_merged_meteo['datetime'].dt.hour

    # Grouper les donn√©es par heure et calculer le total de cyclistes pour chaque heure
    hourly_counts = df_merged_meteo.groupby('Heure')['Comptage'].sum().reset_index()

    # Cr√©ez le graphique en barres avec des ajustements pour les titres et labels
    fig, ax = plt.subplots(figsize=(24, 12))  # Agrandir encore plus la taille de la figure
    ax.bar(hourly_counts['Heure'], hourly_counts['Comptage'], color='skyblue')
    ax.set_xlabel('Heure', fontsize=20)  # Augmenter la taille de la police du label x
    ax.set_ylabel('Nombre', fontsize=20)  # Augmenter la taille de la police du label y
    ax.set_title('Nombre Total de Cyclistes par Heure', fontsize=24)  # Augmenter la taille de la police du titre

    # Ajuster la taille des √©tiquettes des axes x et y
    ax.tick_params(axis='x', labelsize=16)
    ax.tick_params(axis='y', labelsize=16)

    plt.xticks(range(0, 24))
    plt.tight_layout()

    st.pyplot(fig)

    st.write("")
    st.write("")

    st.write("Nous observons que le nombre de cyclistes varie en fonction de l'heure de la journ√©e. Les heures de pointe, g√©n√©ralement le matin et en fin de journ√©e, montrent une augmentation significative du nombre de cyclistes.")

    st.write("### Influence de la Quantit√© de Pluie sur le Trafic Cycliste")

    st.write("""
    Nous allons maintenant analyser l'influence de la quantit√© de pluie tomb√©e sur le trafic cycliste. Nous filtrons les donn√©es pour n'inclure que les heures o√π la quantit√© de pluie est inf√©rieur √† 15mm.
    """)

    # Filtrer les donn√©es pour n'inclure que les valeurs o√π la quantit√© de pluie (RR1) est sup√©rieure √† 15
    df_filtered = df_merged_meteo[df_merged_meteo['RR1'] < 15]

    # Cr√©ez le graphique en nuage de points pour visualiser la relation entre la quantit√© de pluie et le nombre de cyclistes
    fig, ax = plt.subplots(figsize=(24, 12))  # Taille de la figure
    scatter = ax.scatter(df_filtered['RR1'], df_filtered['Comptage'], alpha=0.5, color='skyblue')
    ax.set_xlabel('Quantit√© de Pluie (RR1)', fontsize=20)  # Augmenter la taille de la police du label x
    ax.set_ylabel('Nombre de Cyclistes', fontsize=20)  # Augmenter la taille de la police du label y
    ax.set_title('Influence de la Quantit√© de Pluie sur le Trafic Cycliste', fontsize=24)  # Augmenter la taille de la police du titre

    # Ajuster la taille des √©tiquettes des axes x et y
    ax.tick_params(axis='x', labelsize=16)
    ax.tick_params(axis='y', labelsize=16)

    plt.tight_layout()

    # Afficher le graphique dans Streamlit
    st.pyplot(fig)

    st.write("")
    st.write("")

    st.write("""
    Nous observons que plus la quantit√© de pluie augmente, plus il y a une diminution significative du nombre de cyclistes. Cette analyse nous permet de comprendre l'impact important des fortes pluies sur le trafic cycliste.
    """)

    st.write("### Influence de la Temp√©rature sur le Trafic Cycliste")

    st.write("""
    Nous allons maintenant analyser l'influence de la temp√©rature sur le trafic cycliste.
    """)

    # Filtrer les donn√©es pour n'inclure que les valeurs o√π la temp√©rature est valide (par exemple, sup√©rieures √† une valeur minimale si n√©cessaire)
    df_filtered_temp = df_merged_meteo[df_merged_meteo['T'].notnull()]

    # Cr√©ez le graphique en nuage de points pour visualiser la relation entre la temp√©rature et le nombre de cyclistes
    fig, ax = plt.subplots(figsize=(24, 12))  # Taille de la figure
    scatter = ax.scatter(df_filtered_temp['T'], df_filtered_temp['Comptage'], alpha=0.5, color='skyblue')
    ax.set_xlabel('Temp√©rature (¬∞C)', fontsize=20)  # Augmenter la taille de la police du label x
    ax.set_ylabel('Nombre de Cyclistes', fontsize=20)  # Augmenter la taille de la police du label y
    ax.set_title('Influence de la Temp√©rature sur le Trafic Cycliste', fontsize=24)  # Augmenter la taille de la police du titre

    # Ajuster la taille des √©tiquettes des axes x et y
    ax.tick_params(axis='x', labelsize=16)
    ax.tick_params(axis='y', labelsize=16)

    plt.tight_layout()

    # Afficher le graphique dans Streamlit
    st.pyplot(fig)

    st.write("")
    st.write("")

    st.write("""
    Nous observons que la temp√©rature a un impact significatif sur le nombre de cyclistes. En g√©n√©ral, plus la temp√©rature va dans les extr√™mes (trop froid ou trop chaud) plus le nombre de cyliste baisse.
    """)

if page == pages[3]:
    st.write("## üö¥‚Äç‚ôÇÔ∏è Machine Learning üö¥‚Äç‚ôÄÔ∏è")

    st.write("#### Les diff√©rents mod√®les utilis√©s")

    st.write("""
    Nous allons √† pr√©sent proc√©der √† des exp√©rimentations de Machine Learning : arriver anticiper le comptage horaire futur √† partir des donn√©es pass√©es. Nous devrons ici utiliser des m√©thodes de R√©gression puisque nous avons uniquement des variables quantitatives √† notre disposition.
    Pour mener √† bien notre r√©solution de probl√®me de Machine Learning, nous allons essayer les mod√®les suivants :
             
             
    -	Le Decision Tree Regressor
    -	Le XGBoost
    -	Le Random Forest Regressor
    -	Le Gradient Boosting Regressor
             
    Nous allons ensuite, comparer les performances de ces diff√©rents mod√®les afin de s√©l√©ctionner le meilleur.

    """)

    st.write("#### 1/ Le Decision Tree Regressor")

    st.write("###### R√©sultats :")
    st.write("""
    - **Score du jeu d'entra√Ænement du mod√®le utilisant le DecisionTreeRegressor** : 1.0
    - **Score du jeu de test du mod√®le utilisant le DecisionTreeRegressor** : 0.604
    - **Erreur quadratique moyenne (MSE) sur les donn√©es d'entra√Ænement avec DecisionTreeRegressor** : 0.0
    - **Erreur quadratique moyenne (MSE) sur les donn√©es de test avec DecisionTreeRegressor** : 1387.84
    - **Erreur absolue moyenne (MAE) sur les donn√©es d'entra√Ænement avec DecisionTreeRegressor** : 0.0
    - **Erreur absolue moyenne (MAE) sur les donn√©es de test avec DecisionTreeRegressor** : 18.30
    """)

    st.write("""
    ###### Interpr√©tation :
    Les r√©sultats obtenus avec le mod√®le DecisionTreeRegressor montrent une performance parfaite sur les donn√©es d'entra√Ænement, avec un score de 1.0 et une erreur quadratique moyenne (MSE) de 0.0, ce qui signifie que le mod√®le pr√©dit parfaitement les valeurs d'entra√Ænement. Cependant, les performances sur le jeu de test sont nettement moins bonnes, avec un score de 0.604 et une MSE de 1387.84. L'erreur absolue moyenne (MAE) sur le jeu de test est de 18.30.

    Ces r√©sultats sugg√®rent que le mod√®le souffre de surapprentissage (overfitting). En d'autres termes, il s'ajuste trop pr√©cis√©ment aux donn√©es d'entra√Ænement et ne g√©n√©ralise pas bien sur les nouvelles donn√©es. 
    """)

    st.write("#### 2/ Le XGBoost")

    st.write("###### R√©sultats :")
    st.write("""
    - **Score du jeu d'entra√Ænement du mod√®le utilisant le XGBRegressor** : 0.816
    - **Score du jeu de test du mod√®le utilisant le XGBRegressor** : 0.635
    - **Erreur quadratique moyenne (MSE) sur les donn√©es d'entra√Ænement avec XGB** : 815.71
    - **Erreur quadratique moyenne (MSE) sur les donn√©es de test avec XGB** : 1279.51
    - **Erreur absolue moyenne (MAE) sur les donn√©es d'entra√Ænement avec XGB** : 15.31
    - **Erreur absolue moyenne (MAE) sur les donn√©es de test avec XGB** : 20.55
    """)

    st.write("""
    ###### Interpr√©tation :
    Les r√©sultats obtenus avec le mod√®le XGBRegressor montrent des performances raisonnablement bonnes sur les donn√©es d'entra√Ænement avec un score de 0.816 et une erreur quadratique moyenne (MSE) de 815.71. Cela indique que le mod√®le est capable de bien s'ajuster aux donn√©es d'entra√Ænement.

    Cependant, les performances sur le jeu de test sont l√©g√®rement inf√©rieures avec un score de 0.635 et une MSE de 1279.51. L'erreur absolue moyenne (MAE) sur le jeu de test est de 20.55.

    Ces r√©sultats sugg√®rent que le mod√®le XGBRegressor, bien qu'il soit performant, pourrait encore √™tre am√©lior√© pour mieux g√©n√©raliser aux nouvelles donn√©es. 
    """)

    st.write("#### 3/ Le RandomForest Regressor")

    st.write("###### R√©sultats :")
    st.write("""
    - **Score du jeu d'entra√Ænement du mod√®le utilisant le RandomForest** : 0.982
    - **Score du jeu de test du mod√®le utilisant le RandomForest** : 0.711
    - **Erreur quadratique moyenne (MSE) sur les donn√©es d'entra√Ænement avec RandomForest** : 81.11
    - **Erreur quadratique moyenne (MSE) sur les donn√©es de test avec RandomForest** : 1012.11
    - **Erreur absolue moyenne (MAE) sur les donn√©es d'entra√Ænement avec RandomForest** : 4.12
    - **Erreur absolue moyenne (MAE) sur les donn√©es de test avec RandomForest** : 16.09
    """)

    st.write("""
    ###### Interpr√©tation :
    Les r√©sultats obtenus avec le mod√®le RandomForest Regressor montrent des performances tr√®s √©lev√©es sur les donn√©es d'entra√Ænement avec un score de 0.982 et une erreur quadratique moyenne (MSE) de 81.11. Cela indique que le mod√®le s'ajuste extr√™mement bien aux donn√©es d'entra√Ænement.

    Cependant, les performances sur le jeu de test, bien qu'acceptables, sont moins impressionnantes avec un score de 0.711 et une MSE de 1012.11. L'erreur absolue moyenne (MAE) sur le jeu de test est de 16.09.

    Ces r√©sultats sugg√®rent que le mod√®le RandomForest Regressor pourrait b√©n√©ficier d'am√©liorations pour mieux g√©n√©raliser aux nouvelles donn√©es. 
    """)

    st.write("#### 4/ Le Gradient Boosting Regressor")

    st.write("###### R√©sultats :")
    st.write("""
    - **Score du jeu d'entra√Ænement du mod√®le utilisant le GradientBoostingRegressor** : 0.413
    - **Score du jeu de test du mod√®le utilisant le GradientBoostingRegressor** : 0.413
    - **Erreur quadratique moyenne (MSE) sur les donn√©es d'entra√Ænement avec GradientBoostingRegressor** : 2602.62
    - **Erreur quadratique moyenne (MSE) sur les donn√©es de test avec GradientBoostingRegressor** : 2060.56
    - **Erreur absolue moyenne (MAE) sur les donn√©es d'entra√Ænement avec GradientBoostingRegressor** : 29.484025861707728
    - **Erreur absolue moyenne (MAE) sur les donn√©es de test avec GradientBoostingRegressor** : 28.872413273515676
    """)

    st.write("""
    ###### Interpr√©tation :
    Les r√©sultats obtenus avec le mod√®le GradientBoostingRegressor montrent des performances mod√©r√©es sur les donn√©es d'entra√Ænement avec un score de 0.413 et une erreur quadratique moyenne (MSE) de 2602.62. Cela indique que le mod√®le s'ajuste mal aux donn√©es d'entra√Ænement.

    Les performances sur le jeu de test sont similaires avec un score de 0.413 et une MSE de 2060.56. L'erreur absolue moyenne (MAE) sur le jeu de test est de 28.87.

    Ces r√©sultats sugg√®rent que le mod√®le GradientBoostingRegressor ne correspond pas √† notre probl√©matique et ne sera donc pas s√©l√©ctionn√© comme mod√®le retenu.
    """)

    st.write("## Le mod√®le s√©lectionn√© : le Random Forest Regressor")

    st.write("""Au vu des r√©sultats des tests des diff√©rents mod√®le, j'ai choisi le Random Forest Regressor qui me semble √™tre le plus adapt√© √† la probl√©matique.
            Pour optimiser son fonctionnement, j'ai utilis√© un GridSearchCV. Voici, son code et ses r√©sultats :""")
    
    st.write("##### Le code :")
    
    st.code("""from sklearn.model_selection import GridSearchCV, cross_val_score
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.feature_selection import SelectFromModel

    # D√©finition des valeurs √† tester pour chaque hyperparam√®tre
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [10, 20, 30, 40],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4, 8],
        'max_features': ['sqrt', 'log2'],
        'bootstrap': [True, False]
    }

    # Initialisation du mod√®le
    rf = RandomForestRegressor(random_state=42)

    # Initialisation de GridSearchCV avec le mod√®le et la grille d'hyperparam√®tres
    regression_model_random_forest = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)

    # Entra√Ænement du mod√®le avec GridSearchCV
    regression_model_random_forest.fit(X_train, y_train)

    # √âvaluation du mod√®le sur les donn√©es de test
    best_rf = regression_model_random_forest.best_estimator_
    test_score = best_rf.score(X_test, y_test)
    print(f"Meilleur score sur les donn√©es de test : {test_score}")

    # Utilisation de la validation crois√©e pour une √©valuation plus robuste
    cv_scores = cross_val_score(best_rf, X_train, y_train, cv=5)
    print(f"Scores de validation crois√©e : {cv_scores}")
    print(f"Moyenne des scores de validation crois√©e : {cv_scores.mean()}")
    """)

    st.write("#####  Les r√©sultats :")

    st.write("""
    - **Score du jeu d'entra√Ænement du mod√®le utilisant le GradientBoostingRegressor** : 0.9999990830343426
    - **Score du jeu de test du mod√®le utilisant le GradientBoostingRegressor** : 0.7747517995950621
    - **Erreur quadratique moyenne (MSE) sur les donn√©es d'entra√Ænement avec GradientBoostingRegressor** : 0.004063076386127533
    - **Erreur quadratique moyenne (MSE) sur les donn√©es de test avec GradientBoostingRegressor** : 790.0618651281067
    - **Erreur absolue moyenne (MAE) sur les donn√©es d'entra√Ænement avec GradientBoostingRegressor** : 0.0016180217026652648
    - **Erreur absolue moyenne (MAE) sur les donn√©es de test avec GradientBoostingRegressor** : 14.645880811406819
    """)

    st.write("Voyons maintenant une illustration graphique des pr√©dictions par rapport √† ce qu'il s'est pass√© r√©element :")

    y_test = lire_csv('y_test.csv', sep=',')
    pred_test = lire_csv('pred_test.csv', sep=',')

    # Nombre total de valeurs √† afficher
    num_values_to_plot = 7 * 24

    # Convertir y_test et pred_test en arrays numpy
    y_test_array = np.array(y_test)
    pred_test_array = np.array(pred_test)

    # S√©lectionner uniquement les 7*24 premi√®res valeurs
    y_test_array_reduced = y_test_array[:num_values_to_plot]
    pred_test_array_reduced = pred_test_array[:num_values_to_plot]

    # Cr√©er un index pour les donn√©es r√©duites
    index_reduced = np.arange(num_values_to_plot)

    # Cr√©er la figure et les axes
    fig, ax = plt.subplots(figsize=(14, 7))

    # Tracer les valeurs r√©elles
    ax.plot(index_reduced, y_test_array_reduced, label='Valeurs R√©elles', color='blue')

    # Tracer les valeurs pr√©dites
    ax.plot(index_reduced, pred_test_array_reduced, label='Valeurs Pr√©dites', color='red', linestyle='--')

    # Ajouter des √©tiquettes, une l√©gende et un titre
    ax.set_xlabel('Index')
    ax.set_ylabel('Valeur')
    ax.set_title('Comparaison entre Valeurs Pr√©dites et Valeurs R√©elles (7 jours)')
    ax.legend()

    # Afficher le graphique avec Streamlit
    st.pyplot(fig)

if page == pages[4]:
    st.write("## üö¥‚Äç‚ôÇÔ∏è Conclusion üö¥‚Äç‚ôÄÔ∏è")

    st.write("")
    st.write("")

    st.write("""
             

Nous pouvons conclure de nos exp√©rimentations que le Random Forest Regressor semble apporter de bons r√©sultats pour notre probl√®me de machine learning. Pourtant, malgr√© ces r√©sultats encourageants, la visualisation des pr√©dictions faites avec ce mod√®le nous montre ses limites : la pr√©diction semble fiable pendant les heures creuses mais moins pr√©cise pendant les heures de pointe, lorsque le nombre de v√©los compt√©s augmente significativement.

Comme √©nonc√© pr√©c√©demment, pour am√©liorer le mod√®le, j'aurais pu ajouter des variables explicatives √† mon dataset, telles que les vacances scolaires ou les jours de gr√®ve.

L'utilisation de GridSearchCV m'a permis d'am√©liorer sensiblement mon mod√®le. Cependant, j'ai utilis√© un nombre r√©duit de donn√©es (environ 150,000 lignes) et le GridSearch a mis plusieurs heures √† s'ex√©cuter. Je me demande alors s'il serait coh√©rent de l'utiliser sur un volume de donn√©es beaucoup plus important.

Pour de futures am√©liorations, il serait pertinent de consid√©rer des techniques d'optimisation plus avanc√©es, comme RandomizedSearchCV, ou des approches plus rapides comme l'utilisation de mod√®les de machine learning distribu√©s.
    """)

